{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10678,
     "status": "ok",
     "timestamp": 1757406849528,
     "user": {
      "displayName": "H 12",
      "userId": "02595226029458051592"
     },
     "user_tz": -540
    },
    "id": "bRl5SRZGVbXD",
    "outputId": "79bb5ce6-2298-4c1c-c5ae-cb0f55efc15e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "!pip install uv\n",
    "\n",
    "# Latests transformers + vision libraries\n",
    "!uv pip install --system -q -U git+https://github.com/huggingface/transformers\n",
    "!uv pip install --system -q -U timm torchvision Pillow numpy==1.*\n",
    "!uv pip install pycocotools\n",
    "# For training\n",
    "!uv pip install --system -q -U datasets accelerate\n",
    "!uv pip install --system -q -U \"albumentations==1.4.6\" torchmetrics\n",
    "# checkpoint = \"/content/rtdetr-v2-finetune/checkpoint-672\"\n",
    "\n",
    "# parmaeter : 42M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211,
     "referenced_widgets": [
      "9abcff60b65e41cab4f1265ed7dd7c96",
      "ec3ec2ba4cf647919e53f44c88be0857",
      "39457bb03008406388d637a8774b3281",
      "d9d434a89e424ddea8a8fa76c01e0a43",
      "755625ee3e4e4e518795249367e89efd",
      "ecde3630604c4b2cb86d5b2d1e13fbaa",
      "52e54ccc9922443dac8778ffde52c104",
      "b56ebbb82d2949118ca3da73e551a3a0",
      "303c2f369570478fa675b54983fc858f",
      "4f039592b97e49acaa3aa92f04599390",
      "739ad6205c8343bab2c22f3414e23e05",
      "dd503c1a89434e1c9c2ee2851aff3557",
      "4ac2a16ccaa54dca9f410bc6d8b7be07",
      "54160c6a23724f7891477674b166213d",
      "e728ab8a531b4593a4418022cf77aeaf",
      "7df365767e3b4418b68f298ab88d6f8e",
      "5fe3267c0ed646d4aafdd49b3f905a50",
      "5fe9bc6b81be4909be427aac713ab784",
      "18c6dadf974d4258a800c1e351cdb32f",
      "1f4bbe9c69ce484fbb459d74ea2876a8",
      "3dfca09d3f5e499392cbe6ddeea5e585",
      "1e8bc7926a2843499097e2c38fa2e53f"
     ]
    },
    "executionInfo": {
     "elapsed": 13312,
     "status": "ok",
     "timestamp": 1757406886325,
     "user": {
      "displayName": "H 12",
      "userId": "02595226029458051592"
     },
     "user_tz": -540
    },
    "id": "q3GmR799-Kdq",
    "outputId": "5a2c4f40-7ffe-48aa-9da5-d9d056dce038"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efce3682bd0d4f4bacb0e9c619059391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RTDetrForObjectDetection were not initialized from the model checkpoint at PekingU/rtdetr_r50vd and are newly initialized because the shapes did not match:\n",
      "- model.decoder.class_embed.0.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- model.decoder.class_embed.0.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([1, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.1.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- model.decoder.class_embed.1.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([1, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.2.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- model.decoder.class_embed.2.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([1, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.3.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- model.decoder.class_embed.3.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([1, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.4.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- model.decoder.class_embed.4.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([1, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.5.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- model.decoder.class_embed.5.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([1, 256]) in the model instantiated\n",
      "- model.denoising_class_embed.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([2, 256]) in the model instantiated\n",
      "- model.enc_score_head.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- model.enc_score_head.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([1, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from utils import load_models\n",
    "checkpoint = \"PekingU/rtdetr_r50vd\"\n",
    "tmp = load_models(checkpoint)\n",
    "image_processor = tmp['image_processor']\n",
    "train_transform = tmp['train_transformer']\n",
    "validation_transform = tmp['validation_transformer']\n",
    "model = tmp['model']\n",
    "evaluator = tmp['MAPEvaluator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_id': 0, 'image_name': 'K3A_CHN_20161112052404_23', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at 0x7FC0AC4F1070>, 'bboxes': [[15, 114, 72, 18]], 'category': [0]}\n",
      "1\n",
      "{'image_id': 0, 'category_id': 0, 'bbox': (np.float64(425.0), np.float64(114.0), np.float64(72.0), np.float64(18.0)), 'iscrowd': 0, 'area': np.float64(1296.0)}\n",
      "1\n",
      "{'pixel_values': tensor([[[0.3451, 0.3451, 0.3451,  ..., 0.6784, 0.6706, 0.6667],\n",
      "         [0.3412, 0.3569, 0.3608,  ..., 0.6706, 0.6745, 0.6824],\n",
      "         [0.3412, 0.3608, 0.3647,  ..., 0.6627, 0.6706, 0.6824],\n",
      "         ...,\n",
      "         [0.2510, 0.2314, 0.2235,  ..., 0.2314, 0.2314, 0.2314],\n",
      "         [0.2353, 0.2275, 0.2314,  ..., 0.2275, 0.2235, 0.2275],\n",
      "         [0.2157, 0.2196, 0.2431,  ..., 0.2078, 0.2118, 0.2196]],\n",
      "\n",
      "        [[0.2941, 0.2941, 0.2941,  ..., 0.5686, 0.5608, 0.5569],\n",
      "         [0.2902, 0.3059, 0.3059,  ..., 0.5608, 0.5647, 0.5725],\n",
      "         [0.2902, 0.3098, 0.3137,  ..., 0.5569, 0.5686, 0.5765],\n",
      "         ...,\n",
      "         [0.2431, 0.2235, 0.2157,  ..., 0.2235, 0.2235, 0.2235],\n",
      "         [0.2275, 0.2196, 0.2235,  ..., 0.2196, 0.2157, 0.2196],\n",
      "         [0.2078, 0.2118, 0.2353,  ..., 0.2000, 0.2039, 0.2118]],\n",
      "\n",
      "        [[0.1569, 0.1569, 0.1569,  ..., 0.4118, 0.4039, 0.4000],\n",
      "         [0.1529, 0.1686, 0.1725,  ..., 0.4039, 0.4078, 0.4157],\n",
      "         [0.1529, 0.1725, 0.1765,  ..., 0.4000, 0.4118, 0.4196],\n",
      "         ...,\n",
      "         [0.1490, 0.1333, 0.1255,  ..., 0.1333, 0.1333, 0.1333],\n",
      "         [0.1373, 0.1294, 0.1333,  ..., 0.1294, 0.1255, 0.1294],\n",
      "         [0.1176, 0.1216, 0.1412,  ..., 0.1098, 0.1137, 0.1216]]]), 'labels': {'size': tensor([640, 640]), 'image_id': tensor([0]), 'class_labels': tensor([0]), 'boxes': tensor([[0.9004, 0.2402, 0.1406, 0.0352]]), 'area': tensor([2025.]), 'iscrowd': tensor([0]), 'orig_size': tensor([512, 512])}}\n"
     ]
    }
   ],
   "source": [
    "from data_load import loading_data , RTDataset\n",
    "paths = {\n",
    "    'train_json':\"/root/workspace/data/TL_KS_BBOX\",\n",
    "    'train_jpg':\"/root/workspace/data/TS_KS\",\n",
    "    'validation_json':'/root/workspace/data/VL_KS_BBOX',\n",
    "    'validation_jpg':'/root/workspace/data/VS_KS'\n",
    "}\n",
    "train_data = loading_data(paths['train_json'],paths['train_jpg'])\n",
    "print(train_data[0])\n",
    "validation_data = loading_data(paths['validation_json'],paths['validation_jpg'])\n",
    "train_RTdataset = RTDataset(train_data, image_processor, transform=train_transform)\n",
    "print(train_RTdataset[0])\n",
    "validation_RTdataset = RTDataset(validation_data, image_processor, transform=validation_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "id": "tCzxNhNAl775",
    "outputId": "66dff9b8-2f2e-419e-9ee3-7882040b01ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/venv/main/lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 2863, in __getitems__\n    batch = self.__getitem__(keys)\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/workspace/RT_DETR/data_load.py\", line 115, in __getitem__\n    transformed = self.transform(image=image, bboxes=boxes, category=categories)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/venv/main/lib/python3.12/site-packages/albumentations/core/composition.py\", line 228, in __call__\n    p.preprocess(data)\n  File \"/venv/main/lib/python3.12/site-packages/albumentations/core/utils.py\", line 90, in preprocess\n    data[data_name] = self.check_and_convert(data[data_name], rows, cols, direction=\"to\")\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/venv/main/lib/python3.12/site-packages/albumentations/core/utils.py\", line 104, in check_and_convert\n    return self.convert_to_albumentations(data, rows, cols)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/venv/main/lib/python3.12/site-packages/albumentations/core/bbox_utils.py\", line 145, in convert_to_albumentations\n    data = convert_bboxes_to_albumentations(data, self.params.format, rows, cols, check_validity=False)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/venv/main/lib/python3.12/site-packages/albumentations/core/bbox_utils.py\", line 433, in convert_bboxes_to_albumentations\n    return [convert_bbox_to_albumentations(bbox, source_format, rows, cols, check_validity) for bbox in bboxes]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/venv/main/lib/python3.12/site-packages/albumentations/core/bbox_utils.py\", line 344, in convert_bbox_to_albumentations\n    (x_min, y_min, width, height), tail = bbox[:4], bbox[4:]\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: not enough values to unpack (expected 4, got 2)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m      8\u001b[39m training_args = TrainingArguments(\n\u001b[32m      9\u001b[39m     output_dir=out_dir,\n\u001b[32m     10\u001b[39m     num_train_epochs=\u001b[32m60\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     report_to=\u001b[33m\"\u001b[39m\u001b[33mtensorboard\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     25\u001b[39m )\n\u001b[32m     28\u001b[39m trainer = Trainer(\n\u001b[32m     29\u001b[39m     model=model,\n\u001b[32m     30\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     compute_metrics=evaluator,\n\u001b[32m     36\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/trainer.py:2620\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2618\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2619\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2620\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2621\u001b[39m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[32m   2622\u001b[39m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[32m   2623\u001b[39m \u001b[38;5;28mself\u001b[39m.current_gradient_accumulation_steps = \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/trainer.py:5589\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5587\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5588\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5589\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5590\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5591\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/accelerate/data_loader.py:567\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    569\u001b[39m     \u001b[38;5;28mself\u001b[39m.end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1516\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1514\u001b[39m worker_id = \u001b[38;5;28mself\u001b[39m._task_info.pop(idx)[\u001b[32m0\u001b[39m]\n\u001b[32m   1515\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1516\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1551\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data, worker_idx)\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1551\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/_utils.py:769\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    766\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    767\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    768\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mValueError\u001b[39m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/venv/main/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/venv/main/lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 2863, in __getitems__\n    batch = self.__getitem__(keys)\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/workspace/RT_DETR/data_load.py\", line 115, in __getitem__\n    transformed = self.transform(image=image, bboxes=boxes, category=categories)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/venv/main/lib/python3.12/site-packages/albumentations/core/composition.py\", line 228, in __call__\n    p.preprocess(data)\n  File \"/venv/main/lib/python3.12/site-packages/albumentations/core/utils.py\", line 90, in preprocess\n    data[data_name] = self.check_and_convert(data[data_name], rows, cols, direction=\"to\")\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/venv/main/lib/python3.12/site-packages/albumentations/core/utils.py\", line 104, in check_and_convert\n    return self.convert_to_albumentations(data, rows, cols)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/venv/main/lib/python3.12/site-packages/albumentations/core/bbox_utils.py\", line 145, in convert_to_albumentations\n    data = convert_bboxes_to_albumentations(data, self.params.format, rows, cols, check_validity=False)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/venv/main/lib/python3.12/site-packages/albumentations/core/bbox_utils.py\", line 433, in convert_bboxes_to_albumentations\n    return [convert_bbox_to_albumentations(bbox, source_format, rows, cols, check_validity) for bbox in bboxes]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/venv/main/lib/python3.12/site-packages/albumentations/core/bbox_utils.py\", line 344, in convert_bbox_to_albumentations\n    (x_min, y_min, width, height), tail = bbox[:4], bbox[4:]\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: not enough values to unpack (expected 4, got 2)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments , Trainer\n",
    "import torch\n",
    "from data_load import collate_fn\n",
    "\n",
    "\n",
    "\n",
    "out_dir = \"/workspace/RT_50\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    num_train_epochs=60,\n",
    "    max_grad_norm=0.1,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=300,\n",
    "    per_device_train_batch_size=24,\n",
    "    dataloader_num_workers=2,\n",
    "    metric_for_best_model=\"eval_map\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    eval_do_concat_batches=False,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_RTdataset,\n",
    "    eval_dataset=validation_RTdataset,\n",
    "    processing_class=image_processor,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=evaluator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "executionInfo": {
     "elapsed": 2813,
     "status": "ok",
     "timestamp": 1757408566873,
     "user": {
      "displayName": "H 12",
      "userId": "02595226029458051592"
     },
     "user_tz": -540
    },
    "id": "_B0EpT2UlHj1",
    "outputId": "6e8cef17-6702-4413-ab06-51bd58e89263"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_repo = '/content/drive/My Drive/RT_50/checkpoint-2688'\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_repo)\n",
    "model = AutoModelForObjectDetection.from_pretrained(model_repo)\n",
    "image = Image.open('/content/dataset/val/K3A_CHN_20161112052404_15.jpg')\n",
    "inputs = image_processor(images=[image], return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "\n",
    "result = image_processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]\n",
    "\n",
    "for score, label, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )\n",
    "image_with_boxes = image.copy()\n",
    "draw = ImageDraw.Draw(image_with_boxes)\n",
    "\n",
    "for score, label, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    x, y, x2, y2 = tuple(box)\n",
    "    draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n",
    "    text_label = model.config.id2label[label.item()]\n",
    "    draw.text((x, y), f\"{text_label} [ {score.item():.2f} ]\", fill=\"blue\")\n",
    "\n",
    "image_with_boxes"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNcPIQ4mwtYzpN7wwMswaHa",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "18c6dadf974d4258a800c1e351cdb32f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e8bc7926a2843499097e2c38fa2e53f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1f4bbe9c69ce484fbb459d74ea2876a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "303c2f369570478fa675b54983fc858f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "39457bb03008406388d637a8774b3281": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b56ebbb82d2949118ca3da73e551a3a0",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_303c2f369570478fa675b54983fc858f",
      "value": 1
     }
    },
    "3dfca09d3f5e499392cbe6ddeea5e585": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ac2a16ccaa54dca9f410bc6d8b7be07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5fe3267c0ed646d4aafdd49b3f905a50",
      "placeholder": "​",
      "style": "IPY_MODEL_5fe9bc6b81be4909be427aac713ab784",
      "value": "preprocessor_config.json: 100%"
     }
    },
    "4f039592b97e49acaa3aa92f04599390": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52e54ccc9922443dac8778ffde52c104": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "54160c6a23724f7891477674b166213d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18c6dadf974d4258a800c1e351cdb32f",
      "max": 841,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1f4bbe9c69ce484fbb459d74ea2876a8",
      "value": 841
     }
    },
    "5fe3267c0ed646d4aafdd49b3f905a50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5fe9bc6b81be4909be427aac713ab784": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "739ad6205c8343bab2c22f3414e23e05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "755625ee3e4e4e518795249367e89efd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7df365767e3b4418b68f298ab88d6f8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9abcff60b65e41cab4f1265ed7dd7c96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ec3ec2ba4cf647919e53f44c88be0857",
       "IPY_MODEL_39457bb03008406388d637a8774b3281",
       "IPY_MODEL_d9d434a89e424ddea8a8fa76c01e0a43"
      ],
      "layout": "IPY_MODEL_755625ee3e4e4e518795249367e89efd"
     }
    },
    "b56ebbb82d2949118ca3da73e551a3a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9d434a89e424ddea8a8fa76c01e0a43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f039592b97e49acaa3aa92f04599390",
      "placeholder": "​",
      "style": "IPY_MODEL_739ad6205c8343bab2c22f3414e23e05",
      "value": " 1/1 [00:00&lt;00:00,  4.90it/s]"
     }
    },
    "dd503c1a89434e1c9c2ee2851aff3557": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4ac2a16ccaa54dca9f410bc6d8b7be07",
       "IPY_MODEL_54160c6a23724f7891477674b166213d",
       "IPY_MODEL_e728ab8a531b4593a4418022cf77aeaf"
      ],
      "layout": "IPY_MODEL_7df365767e3b4418b68f298ab88d6f8e"
     }
    },
    "e728ab8a531b4593a4418022cf77aeaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3dfca09d3f5e499392cbe6ddeea5e585",
      "placeholder": "​",
      "style": "IPY_MODEL_1e8bc7926a2843499097e2c38fa2e53f",
      "value": " 841/841 [00:00&lt;00:00, 77.0kB/s]"
     }
    },
    "ec3ec2ba4cf647919e53f44c88be0857": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ecde3630604c4b2cb86d5b2d1e13fbaa",
      "placeholder": "​",
      "style": "IPY_MODEL_52e54ccc9922443dac8778ffde52c104",
      "value": "Fetching 1 files: 100%"
     }
    },
    "ecde3630604c4b2cb86d5b2d1e13fbaa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
