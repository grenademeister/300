{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2d50f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q git+https://github.com/huggingface/transformers.git\n",
    "%pip install -q matplotlib rasterio numpy requests torch scipy albumentations\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "import warnings\n",
    "from rasterio.errors import NotGeoreferencedWarning\n",
    "warnings.filterwarnings(\"ignore\", category=NotGeoreferencedWarning)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation, Mask2FormerConfig\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import albumentations as A\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5940cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.8.0+cu128  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400bf98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"/root/workspace/train/TS_SN10_SN10\"\n",
    "BINS = 8192\n",
    "LOW_Q, HIGH_Q = 0.0001, 0.9999\n",
    "\n",
    "gmin = np.array([np.inf, np.inf, np.inf], dtype=np.float64)\n",
    "gmax = np.array([-np.inf, -np.inf, -np.inf], dtype=np.float64)\n",
    "\n",
    "tif_files = [f for f in sorted(os.listdir(img_dir)) if f.endswith(\".tif\")]\n",
    "for fname in tif_files:\n",
    "    fpath = os.path.join(img_dir, fname)\n",
    "    with rasterio.open(fpath) as src:\n",
    "        r = src.read(1).astype(np.float64)\n",
    "        g = src.read(2).astype(np.float64)\n",
    "        b = src.read(3).astype(np.float64)\n",
    "\n",
    "    gmin = np.minimum(gmin, [r.min(), g.min(), b.min()])\n",
    "    gmax = np.maximum(gmax, [r.max(), g.max(), b.max()])\n",
    "\n",
    "print(\"[global min]\", gmin)\n",
    "print(\"[global max]\", gmax)\n",
    "\n",
    "eps = 1e-9\n",
    "gmax = np.maximum(gmax, gmin + eps)\n",
    "\n",
    "edges_r = np.linspace(gmin[0], gmax[0], BINS + 1)\n",
    "edges_g = np.linspace(gmin[1], gmax[1], BINS + 1)\n",
    "edges_b = np.linspace(gmin[2], gmax[2], BINS + 1)\n",
    "\n",
    "hist_r = np.zeros(BINS, dtype=np.int64)\n",
    "hist_g = np.zeros(BINS, dtype=np.int64)\n",
    "hist_b = np.zeros(BINS, dtype=np.int64)\n",
    "\n",
    "for fname in tif_files:\n",
    "    fpath = os.path.join(img_dir, fname)\n",
    "    with rasterio.open(fpath) as src:\n",
    "        r = src.read(1).astype(np.float64).ravel()\n",
    "        g = src.read(2).astype(np.float64).ravel()\n",
    "        b = src.read(3).astype(np.float64).ravel()\n",
    "\n",
    "    h_r, _ = np.histogram(r, bins=edges_r)\n",
    "    h_g, _ = np.histogram(g, bins=edges_g)\n",
    "    h_b, _ = np.histogram(b, bins=edges_b)\n",
    "    hist_r += h_r; hist_g += h_g; hist_b += h_b\n",
    "\n",
    "total_r = hist_r.sum()\n",
    "total_g = hist_g.sum()\n",
    "total_b = hist_b.sum()\n",
    "\n",
    "def q_from_hist(hist, edges, q):\n",
    "    c = np.cumsum(hist)\n",
    "    thresh = q * c[-1]\n",
    "    idx = np.searchsorted(c, thresh, side=\"left\")\n",
    "    idx = np.clip(idx, 0, len(edges) - 2)\n",
    "    return edges[idx]\n",
    "\n",
    "r_min = q_from_hist(hist_r, edges_r, LOW_Q)\n",
    "r_max = q_from_hist(hist_r, edges_r, HIGH_Q)\n",
    "g_min = q_from_hist(hist_g, edges_g, LOW_Q)\n",
    "g_max = q_from_hist(hist_g, edges_g, HIGH_Q)\n",
    "b_min = q_from_hist(hist_b, edges_b, LOW_Q)\n",
    "b_max = q_from_hist(hist_b, edges_b, HIGH_Q)\n",
    "\n",
    "print(f\"Red: {r_min:.3f}, {r_max:.3f}\")\n",
    "print(f\"Green: {g_min:.3f}, {g_max:.3f}\")\n",
    "print(f\"Blue: {b_min:.3f}, {b_max:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f903d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_min = 1121.969\n",
    "r_max = 4942.150\n",
    "g_min = 1207.664\n",
    "g_max = 4702.109\n",
    "b_min = 1053.633\n",
    "b_max = 4507.207\n",
    "\n",
    "# r_min = 1045.748\n",
    "# r_max = 6740.959\n",
    "# g_min = 1102.797\n",
    "# g_max = 6511.914\n",
    "# b_min = 959.977\n",
    "# b_max = 6216.434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8897f2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86815a3dcf0941c2959f218141057852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "/venv/main/lib/python3.12/site-packages/transformers/image_processing_base.py:410: UserWarning: The following named arguments are not valid for `Mask2FormerImageProcessor.__init__` and were ignored: '_max_size', 'reduce_labels'\n",
      "  image_processor = cls(**image_processor_dict)\n",
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-large-cityscapes-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.bias: found shape torch.Size([20]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- class_predictor.weight: found shape torch.Size([20, 256]) in the checkpoint and torch.Size([3, 256]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([20]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-large-cityscapes-semantic\")\n",
    "id2label = {0: \"background\", 1: \"target\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "config = Mask2FormerConfig.from_pretrained(\n",
    "    \"facebook/mask2former-swin-large-cityscapes-semantic\",\n",
    "    id2label=id2label, label2id=label2id, num_labels=len(id2label)\n",
    ")\n",
    "config.backbone_config.drop_rate = 0.1\n",
    "config.backbone_config.attn_drop_rate = 0.1\n",
    "config.backbone_config.drop_path_rate = 0.1\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
    "    \"facebook/mask2former-swin-large-cityscapes-semantic\",\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0716b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_343029/1496344693.py:28: UserWarning: Argument(s) 'mean' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(std_range=(0.0, 0.005), mean=0.0, p=1.0),\n"
     ]
    }
   ],
   "source": [
    "def to_float32(img, **kwargs):\n",
    "    return img.astype(np.float32, copy=False)\n",
    "\n",
    "def keep_mask_int(msk, **kwargs):\n",
    "    if msk.dtype != np.int64:\n",
    "        msk = msk.astype(np.int64, copy=False)\n",
    "    return msk\n",
    "\n",
    "def clip01(img, **kwargs):\n",
    "    return np.clip(img, 0.0, 1.0)\n",
    "\n",
    "H, W = 512, 512\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Lambda(image=to_float32, mask=keep_mask_int),\n",
    "\n",
    "    \n",
    "    # A.OneOf([\n",
    "    #     A.RandomResizedCrop(size=(H, W), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "    #     A.Resize(H, W)\n",
    "    # ], p=1.0),\n",
    "\n",
    "    # A.HorizontalFlip(p=0.5),\n",
    "    # A.VerticalFlip(p=0.5),\n",
    "    # A.RandomRotate90(p=1.0),\n",
    "    \n",
    "    A.OneOf([\n",
    "        A.GaussNoise(std_range=(0.0, 0.005), mean=0.0, p=1.0),\n",
    "        A.MultiplicativeNoise(multiplier=(0.95, 1.05), per_channel=True, p=1.0),\n",
    "    ], p=0.3),\n",
    "    # A.OneOf([\n",
    "    #     A.GaussianBlur(blur_limit=(2, 3), p=1.0),\n",
    "    #     A.MotionBlur(blur_limit=3, p=1.0),\n",
    "    # ], p=0.3),\n",
    "    \n",
    "    # A.OneOf([\n",
    "    #     A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=1.0),\n",
    "    #     A.RandomGamma(gamma_limit=(80, 120), p=1.0),\n",
    "    #     A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=1.0),\n",
    "    # ], p=0.5),\n",
    "    \n",
    "    A.Lambda(image=clip01),\n",
    "])\n",
    "\n",
    "\n",
    "def augmentation(image, mask):\n",
    "    # image: (H,W,3), float32, range (0,1)\n",
    "    # mask : (H,W), int, 0/1\n",
    "    out = transform(image=image, mask=mask)\n",
    "    img_aug, msk_aug = out[\"image\"], out[\"mask\"]\n",
    "    return img_aug, msk_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "184b7754",
   "metadata": {},
   "outputs": [],
   "source": [
    "##label은 10/90으로 구성된다... 10이 검은색 /  90이 노란색\n",
    "\n",
    "class SatelliteImageDataset(Dataset):\n",
    "  def __init__(self, img_dir, label_dir, transform=None):\n",
    "    self.img_dir   = img_dir\n",
    "    self.label_dir = label_dir\n",
    "    self.transform = transform\n",
    "\n",
    "    self.img_files   = sorted([f for f in os.listdir(img_dir) if f.endswith('.tif')])\n",
    "    self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith('.tif')])\n",
    "    assert len(self.img_files) == len(self.label_files), \"data number mismatch\"\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.img_files)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    img_path = os.path.join(self.img_dir, self.img_files[idx])\n",
    "    label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "\n",
    "    with rasterio.open(img_path) as src:\n",
    "      red   = src.read(1).astype(np.float32)\n",
    "      green = src.read(2).astype(np.float32)\n",
    "      blue  = src.read(3).astype(np.float32)\n",
    "\n",
    "      red   = (red   - r_min) / (r_max - r_min)\n",
    "      green = (green - g_min) / (g_max - g_min)\n",
    "      blue  = (blue  - b_min) / (b_max - b_min)\n",
    "\n",
    "      image = np.dstack((red, green, blue))\n",
    "      image = image.clip(0, 1)\n",
    "\n",
    "    with rasterio.open(label_path) as src:\n",
    "      label = src.read(1)\n",
    "      label = (label==10).astype(np.int64)\n",
    "\n",
    "    if self.transform:\n",
    "      image, label = self.transform(image, label)\n",
    "\n",
    "    image = torch.from_numpy(image).float().permute(2,0,1)  #(H,W,C) => (C,H,W)\n",
    "    label = torch.from_numpy(label).long()\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd13fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs  = [img.permute(1,2,0).numpy() for (img, msk) in batch]     # CHW -> HWC float32[0,1]\n",
    "    masks = [msk.numpy().astype(np.int64) for (img, msk) in batch]   # HW int64\n",
    "\n",
    "    enc = processor(\n",
    "        images=imgs,\n",
    "        segmentation_maps=masks,\n",
    "        return_tensors=\"pt\",\n",
    "        size={\"height\": 512, \"width\": 512},\n",
    "        do_rescale=False,\n",
    "    )\n",
    "    return enc   # keys: pixel_values, mask_labels(list), class_labels(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92bd437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter set\n",
    "batch_size = 7\n",
    "lr = 0.0001\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a44b3646",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data   = SatelliteImageDataset(img_dir='/root/workspace/train/TS_SN10_SN10',label_dir='/root/workspace/train/TL_SN10',transform=augmentation)\n",
    "val_data     = SatelliteImageDataset(img_dir='/root/workspace/valid/VS_SN10_SN10',label_dir='/root/workspace/valid/VL_SN10',transform=None)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b12a05ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = model.config.num_labels # 2\n",
    "\n",
    "@torch.no_grad()\n",
    "def pred_from_outputs(outputs, H, W, num_classes):\n",
    "    # outputs.class_queries_logits: (B,Q,C+1), outputs.masks_queries_logits: (B,Q,h',w')\n",
    "    class_logits = outputs.class_queries_logits\n",
    "    mask_logits  = outputs.masks_queries_logits\n",
    "    class_scores = class_logits.softmax(-1)[..., :num_classes]    # drop no-object\n",
    "    mask_probs   = mask_logits.sigmoid()\n",
    "    # (B,Q,C) x (B,Q,h',w') -> (B,C,h',w')\n",
    "    sem_lowres   = torch.einsum('bqc,bqhw->bchw', class_scores, mask_probs)\n",
    "    sem          = torch.nn.functional.interpolate(sem_lowres, size=(H, W), mode='bilinear', align_corners=False)\n",
    "    pred         = sem.argmax(1)                          # (B,H,W), GPU\n",
    "    return pred\n",
    "\n",
    "@torch.no_grad()\n",
    "def sem_probs_from_outputs(outputs, H, W, num_classes):\n",
    "    class_scores = outputs.class_queries_logits.softmax(-1)[..., :num_classes]  # (B,Q,C)\n",
    "    mask_probs   = outputs.masks_queries_logits.sigmoid()                       # (B,Q,h',w')\n",
    "    sem_lowres   = torch.einsum('bqc,bqhw->bchw', class_scores, mask_probs)     # (B,C,h',w')\n",
    "    sem          = F.interpolate(sem_lowres, size=(H, W), mode='bilinear', align_corners=False)\n",
    "    return sem  # (B,C,H,W), 확률 해석(0~1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def m2f_sem_probs_from_outputs(m2f_outputs, H, W, num_classes=2):\n",
    "    class_scores = m2f_outputs.class_queries_logits.softmax(-1)[..., :num_classes]  # (B,Q,C)\n",
    "    mask_probs   = m2f_outputs.masks_queries_logits.sigmoid()                       # (B,Q,h',w')\n",
    "    sem_lowres   = torch.einsum('bqc,bqhw->bchw', class_scores, mask_probs)         # (B,C,h',w')\n",
    "    sem          = F.interpolate(sem_lowres, size=(H, W), mode='bilinear', align_corners=False)\n",
    "    return sem.clamp(0.0, 1.0)  # (B,C,H,W)\n",
    "\n",
    "@torch.no_grad()\n",
    "def m2f_tta_probs(model, pixel_values, num_classes):\n",
    "    B, C, H, W = pixel_values.shape\n",
    "    probs_acc, nviews = None, 0\n",
    "\n",
    "    def fwd(x, undo):\n",
    "        nonlocal probs_acc, nviews\n",
    "        x = x.contiguous()\n",
    "        output = model(pixel_values=x)\n",
    "        prob = m2f_sem_probs_from_outputs(output, H, W, num_classes)  # (B,C,out_H,out_W)\n",
    "        prob = undo(prob)\n",
    "        probs_acc = prob if probs_acc is None else probs_acc + prob\n",
    "        nviews += 1\n",
    "\n",
    "    x = pixel_values\n",
    "    out_H, out_W = H, W\n",
    "    fwd(x, undo=lambda t: t) # identity\n",
    "    fwd(x.flip(-1), undo=lambda t: t.flip(-1)) # hflip\n",
    "    fwd(x.flip(-2), undo=lambda t: t.flip(-2)) # vflip\n",
    "    fwd(x.transpose(-1, -2), undo=lambda t: t.transpose(-1, -2)) # transpose\n",
    "\n",
    "    probs_mean = probs_acc / float(nviews)          # (B,C,H,W)\n",
    "    return probs_mean\n",
    "\n",
    "\n",
    "def build_targets(mask_labels_list, class_labels_list, H, W, device):\n",
    "    tgts = []\n",
    "    for masks, classes in zip(mask_labels_list, class_labels_list):\n",
    "        tgt = torch.zeros((H, W), dtype=torch.long, device=device)\n",
    "        if masks.ndim == 3:\n",
    "            for m, c in zip(masks, classes):\n",
    "                tgt[m.bool()] = int(c.item())\n",
    "        tgts.append(tgt)\n",
    "    return tgts\n",
    "\n",
    "@torch.no_grad()\n",
    "def accumulate_iou(pred, tgt, num_classes):\n",
    "    inter = torch.zeros(num_classes, dtype=torch.float64, device=pred.device)\n",
    "    union = torch.zeros(num_classes, dtype=torch.float64, device=pred.device)\n",
    "    for c in range(num_classes):\n",
    "        p = (pred == c); t = (tgt == c)\n",
    "        inter[c] += (p & t).sum().double()\n",
    "        union[c] += (p | t).sum().double()\n",
    "    return inter, union\n",
    "\n",
    "def miou_from_inter_union(inter, union):\n",
    "    iou = inter / (union + 1e-10)\n",
    "    valid = union > 0\n",
    "    return (iou[valid].mean().item() if valid.any() else float(\"nan\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "\n",
    "# 1) 큰 객체 먼저 CPU로 옮기고 참조 해제\n",
    "try:\n",
    "    model.to(\"cpu\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 필요시 중간 텐서/배치도 삭제\n",
    "for name in [\n",
    "    \"model\",\"optimizer\",\"scheduler\",\"outputs\",\"loss\",\n",
    "    \"pixel_values\",\"mask_labels\",\"class_labels\",\n",
    "    \"train_loader\",\"val_loader\",\"train_data\",\"val_data\",\n",
    "]:\n",
    "    if name in globals():\n",
    "        del globals()[name]\n",
    "\n",
    "# 2) 가비지 컬렉션\n",
    "gc.collect()\n",
    "\n",
    "# 3) CUDA 캐시 정리\n",
    "torch.cuda.empty_cache()      # PyTorch 캐시 해제\n",
    "torch.cuda.ipc_collect()      # 프로세스 간 공유 메모리 회수\n",
    "torch.cuda.reset_peak_memory_stats()  # 피크 메모리 통계 초기화(선택)\n",
    "\n",
    "# 5) 확인(선택)\n",
    "if torch.cuda.is_available():\n",
    "    dev = torch.cuda.current_device()\n",
    "    print(\"allocated:\", torch.cuda.memory_allocated(dev)/1024**2, \"MB\")\n",
    "    print(\"reserved: \", torch.cuda.memory_reserved(dev)/1024**2, \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd83ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147bdfc23c1647aabcca02cc7a7d4e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 1/30:   0%|          | 0/1143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     49\u001b[39m loss.backward()\n\u001b[32m     50\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m train_running += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# ---- mIoU ----\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step % miou_every == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "miou_every = 10  # 학습 중 mIoU 계산 빈도\n",
    "start_epoch = 0  # 기본 시작 epoch\n",
    "\n",
    "model.to(DEVICE)\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "scheduler=CosineAnnealingLR(optimizer, T_max=30, eta_min=lr/10)\n",
    "\n",
    "checkpoint_path = '/root/workspace/models/Mask2Former_model_noise_2.pth'\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "  checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "  model.load_state_dict(checkpoint['model'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "  scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "  start_epoch = checkpoint['epoch'] + 1\n",
    "  train_losses = checkpoint.get('train_losses', [])\n",
    "  val_losses = checkpoint.get('val_losses', [])\n",
    "  train_mIoUs = checkpoint.get('train_mIoU', [])\n",
    "  val_mIoUs = checkpoint.get('val_mIoU', [])\n",
    "  print(f\"Resuming training from epoch {start_epoch}\")\n",
    "else:\n",
    "  train_losses = []\n",
    "  val_losses = []\n",
    "  train_mIoUs = []\n",
    "  val_mIoUs = []\n",
    "\n",
    "for epoch in range(start_epoch):\n",
    "  print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {train_losses[epoch]:.4f} | Train mIoU: {train_mIoUs[epoch]:.4f} | Val Loss: {val_losses[epoch]:.4f} | Val mIoU: {val_mIoUs[epoch]:.4f}\")\n",
    "\n",
    "for epoch in range(start_epoch,num_epochs):\n",
    "    # ---------- Train ----------\n",
    "    model.train()\n",
    "    train_running = 0.0\n",
    "    train_inter = torch.zeros(NUM_CLASSES, dtype=torch.float64, device=DEVICE)\n",
    "    train_union = torch.zeros(NUM_CLASSES, dtype=torch.float64, device=DEVICE)\n",
    "    train_bar = tqdm(train_loader, desc=f\"Train {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "    for step, batch in enumerate(train_bar, 1):\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        mask_labels  = [x.to(DEVICE) for x in batch[\"mask_labels\"]]\n",
    "        class_labels = [x.to(DEVICE) for x in batch[\"class_labels\"]]\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values,\n",
    "                        mask_labels=mask_labels,\n",
    "                        class_labels=class_labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_running += loss.item()\n",
    "\n",
    "        # ---- mIoU ----\n",
    "        if step % miou_every == 0:\n",
    "          H, W = pixel_values.shape[-2], pixel_values.shape[-1]\n",
    "          B = pixel_values.size(0)\n",
    "          with torch.no_grad():\n",
    "              # prob = m2f_sem_probs_from_outputs(outputs, H, W, NUM_CLASSES)\n",
    "              # pred = (prob[:, 1] >= 0.5).long()\n",
    "              pred = processor.post_process_semantic_segmentation(\n",
    "                  outputs, target_sizes=[(H, W)] * B\n",
    "              )\n",
    "              tgts = build_targets(mask_labels, class_labels, H, W, device=pixel_values.device)\n",
    "              for p, t in zip(pred, tgts):\n",
    "                  p = p.to(t.device)\n",
    "                  inter, uni = accumulate_iou(p, t, NUM_CLASSES)\n",
    "                  train_inter += inter; train_union += uni\n",
    "              del pred, tgts\n",
    "          train_mIoU = miou_from_inter_union(train_inter, train_union)\n",
    "          train_bar.set_postfix(loss=f\"{train_running/step:.4f}\", mIoU=f\"{train_mIoU:.4f}\")\n",
    "        else:\n",
    "            train_bar.set_postfix(loss=f\"{train_running/step:.4f}\")\n",
    "\n",
    "    train_epoch_loss = train_running / len(train_loader)\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    train_mIoU = miou_from_inter_union(train_inter, train_union)\n",
    "    train_mIoUs.append(train_mIoU)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "    # ---------- Validation ----------\n",
    "    model.eval()\n",
    "    val_epoch_loss = float(\"inf\")\n",
    "    val_mIoU = float(\"nan\")\n",
    "    if val_loader is not None:\n",
    "        val_running = 0.0\n",
    "        val_inter = torch.zeros(NUM_CLASSES, dtype=torch.float64, device=DEVICE)\n",
    "        val_union = torch.zeros(NUM_CLASSES, dtype=torch.float64, device=DEVICE)\n",
    "        val_bar = tqdm(val_loader, desc=f\"Val   {epoch+1}/{num_epochs}\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for step, batch in enumerate(val_bar, 1):\n",
    "                pv = batch[\"pixel_values\"].to(DEVICE)\n",
    "                ml = [x.to(DEVICE) for x in batch[\"mask_labels\"]]\n",
    "                cl = [x.to(DEVICE) for x in batch[\"class_labels\"]]\n",
    "\n",
    "                outputs = model(pixel_values=pv, mask_labels=ml, class_labels=cl)\n",
    "                val_running += outputs.loss.item()\n",
    "\n",
    "                H, W = pv.shape[-2], pv.shape[-1]\n",
    "                B = pv.size(0)\n",
    "                # prob = m2f_sem_probs_from_outputs(outputs, H, W, NUM_CLASSES)\n",
    "                # pred = (prob[:, 1] >= 0.5).long()\n",
    "                pred = processor.post_process_semantic_segmentation(\n",
    "                    outputs, target_sizes=[(H, W)] * B\n",
    "                )\n",
    "                tgts = build_targets(ml, cl, H, W, device=pv.device)\n",
    "                for p, t in zip(pred, tgts):\n",
    "                    p = p.to(t.device)\n",
    "                    inter, uni = accumulate_iou(p, t, NUM_CLASSES)\n",
    "                    val_inter += inter; val_union += uni\n",
    "                del pred, tgts\n",
    "                cur_miou = miou_from_inter_union(val_inter, val_union)\n",
    "                val_bar.set_postfix(loss=f\"{val_running/step:.4f}\", mIoU=f\"{cur_miou:.4f}\")\n",
    "\n",
    "        val_epoch_loss = val_running / len(val_loader)\n",
    "        val_losses.append(val_epoch_loss)\n",
    "        val_mIoU = miou_from_inter_union(val_inter, val_union)\n",
    "        val_mIoUs.append(val_mIoU)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "          f\"train_loss={train_epoch_loss:.4f} | train_mIoU={train_mIoU:.4f} | \"\n",
    "          f\"val_loss={val_epoch_loss:.4f} | val_mIoU={val_mIoU:.4f}\")\n",
    "    torch.save({'model':model.state_dict(),'optimizer':optimizer.state_dict(),'scheduler':scheduler.state_dict(),\n",
    "                'epoch':epoch,'train_losses': train_losses,'val_losses': val_losses,\n",
    "                'train_mIoU':train_mIoUs,'val_mIoU':val_mIoUs}, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a61e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20,6))\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "# Train loss와 Validation loss를 각각 그래프에 추가\n",
    "ax[0].plot(epochs, train_losses, 'b-o', label='Train Loss')\n",
    "ax[0].plot(epochs, val_losses, 'r-s', label='Validation Loss')\n",
    "\n",
    "ax[1].plot(epochs, train_mIoUs, 'b-o', label='Train mIoU')\n",
    "ax[1].plot(epochs, val_mIoUs, 'r-s', label='Validation mIoU')\n",
    "\n",
    "# 그래프 제목, x축, y축 레이블 추가\n",
    "ax[0].set_title('Training & Validation Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "\n",
    "ax[1].set_title('Training & Validation mIoU')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('mIoU')\n",
    "\n",
    "\n",
    "# 범례(legend) 및 그리드(grid) 추가\n",
    "ax[0].legend()\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].legend()\n",
    "ax[1].grid(True)\n",
    "\n",
    "# 그래프를 화면에 바로 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d0f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '/root/workspace/train/TS_SN10_SN10/SN10_CHN_00003_230409.tif'\n",
    "label_name = '/root/workspace/train/TL_SN10/SN10_CHN_00003_230409.tif'\n",
    "with rasterio.open(file_name) as src:\n",
    "    # 시각화를 위해 RGB 밴드를 읽어옵니다. (밴드 1, 2, 3)\n",
    "    red = src.read(1)\n",
    "    green = src.read(2)\n",
    "    blue = src.read(3)\n",
    "    red   = (red   - r_min)   / (r_max   - r_min)\n",
    "    green = (green - g_min) / (g_max - g_min)\n",
    "    blue  = (blue  - b_min)  / (b_max  - b_min)\n",
    "    # Matplotlib에서 RGB 이미지를 표시하기 위해 (높이, 너비, 밴드) 형태로 배열을 재구성합니다.\n",
    "    img = np.dstack((red, green, blue))\n",
    "    img = img.clip(0, 1)\n",
    "with rasterio.open(label_name) as src:\n",
    "    label = src.read(1)\n",
    "    label = (label==90).astype(np.int64)\n",
    "# img, label = augmentation(img, label)\n",
    "inputs = processor(images=img, return_tensors=\"pt\", do_rescale=False)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20,6))\n",
    "\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "ax[0].imshow(img)\n",
    "ax[1].imshow(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f018b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label: 0/1, img: float in [0,1], shape (H, W, 3)\n",
    "mask1 = (label == 1)\n",
    "mask0 = (label == 0)\n",
    "\n",
    "# 채널별 평탄화된 벡터 얻기\n",
    "R = img[..., 0]; G = img[..., 1]; B = img[..., 2]\n",
    "R1, G1, B1 = R[mask1], G[mask1], B[mask1]\n",
    "R0, G0, B0 = R[mask0], G[mask0], B[mask0]\n",
    "\n",
    "# 통계(평균/표준편차) 계산 (비교용)\n",
    "stats = {\n",
    "    \"R(mask=1)\": (float(np.mean(R1)), float(np.std(R1))),\n",
    "    \"R(mask=0)\": (float(np.mean(R0)), float(np.std(R0))),\n",
    "    \"G(mask=1)\": (float(np.mean(G1)), float(np.std(G1))),\n",
    "    \"G(mask=0)\": (float(np.mean(G0)), float(np.std(G0))),\n",
    "    \"B(mask=1)\": (float(np.mean(B1)), float(np.std(B1))),\n",
    "    \"B(mask=0)\": (float(np.mean(B0)), float(np.std(B0))),\n",
    "}\n",
    "print(\"채널별 통계 (mean, std):\")\n",
    "for k, v in stats.items():\n",
    "    print(f\"  {k}: mean={v[0]:.4f}, std={v[1]:.4f}\")\n",
    "\n",
    "# 히스토그램 설정\n",
    "bins = np.linspace(0, 1, 101)  # 0~1 구간 50개 bin\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4), sharey=True)\n",
    "titles = [\"Red\", \"Green\", \"Blue\"]\n",
    "pairs = [(\"R\", R1, R0), (\"G\", G1, G0), (\"B\", B1, B0)]\n",
    "\n",
    "for ax, (title, ch1, ch0) in zip(axes, pairs):\n",
    "    ax.hist(ch0, bins=bins, histtype='step', density=True, linewidth=1.8, label='mask=0')\n",
    "    ax.hist(ch1, bins=bins, histtype='step', density=True, linewidth=1.8, label='mask=1')\n",
    "    ax.set_title(f\"{title} channel\")\n",
    "    ax.set_xlabel(\"pixel value (0~1)\")\n",
    "    ax.set_ylabel(\"density\")\n",
    "    ax.grid(alpha=0.25)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ec1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(**inputs.to(DEVICE))\n",
    "    Cp1 = out.class_queries_logits.shape[-1]  # = C_head + 1 (no-object 포함)\n",
    "print(\"head classes (without no-object) =\", Cp1-1,\n",
    "      \"| config.num_labels =\", model.config.num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541cff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pred_from_outputs(outputs, H, W, num_classes):\n",
    "    # outputs.class_queries_logits: (B,Q,C+1), outputs.masks_queries_logits: (B,Q,h',w')\n",
    "    class_logits = outputs.class_queries_logits\n",
    "    mask_logits  = outputs.masks_queries_logits\n",
    "    class_scores = class_logits.softmax(-1)[..., :num_classes]    # drop no-object\n",
    "    mask_probs   = mask_logits.sigmoid()\n",
    "    # (B,Q,C) x (B,Q,h',w') -> (B,C,h',w')\n",
    "    sem_lowres   = torch.einsum('bqc,bqhw->bchw', class_scores, mask_probs)\n",
    "    sem          = torch.nn.functional.interpolate(sem_lowres, size=(H, W), mode='bilinear', align_corners=False)\n",
    "    pred         = sem.argmax(1)                          # (B,H,W), GPU\n",
    "    return pred\n",
    "\n",
    "H, W = img.shape[:2]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    enc = processor(images=[img], return_tensors=\"pt\", do_rescale=False, size={\"height\": 512, \"width\": 512})\n",
    "\n",
    "    \n",
    "pixel_values = enc[\"pixel_values\"].to(DEVICE) # (1,3,H,W)\n",
    "prob = m2f_tta_probs(\n",
    "    model, pixel_values, NUM_CLASSES\n",
    ")\n",
    "pred = prob[:, 1] >= 0.5\n",
    "pred = pred.detach().cpu().numpy().astype(np.uint8)\n",
    "\n",
    "palette_vals = np.array([10, 90], dtype=np.uint8)\n",
    "color_seg = palette_vals[pred]          # (H,W), 10/90 단채널\n",
    "\n",
    "import rasterio\n",
    "with rasterio.open(label_name) as src:\n",
    "    img_label = src.read(1).astype(np.uint16)  # {10,90}\n",
    "\n",
    "# ---- 시각화: 연속형 cmap 말고 이산 팔레트로 확정 표시\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\n",
    "# 10=검정, 90=노랑을 시각적으로 분명히 보여주려면, 0/1 인덱스로 바꿔 cmap 적용\n",
    "seg_idx = (color_seg == 10).transpose(1, 2, 0).astype(np.uint8)\n",
    "gt_idx  = (img_label == 90).astype(np.uint8)\n",
    "\n",
    "cmap = ListedColormap([[0,0,0], [1,1,0]])  # 0=검정, 1=노랑\n",
    "norm = BoundaryNorm([-0.5, 0.5, 1.5], cmap.N)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,6))\n",
    "ax[0].imshow(seg_idx, cmap=cmap, norm=norm); ax[0].set_title(\"Prediction (matched path)\"); ax[0].axis('off')\n",
    "ax[1].imshow(gt_idx,  cmap=cmap, norm=norm); ax[1].set_title(\"Ground Truth\"); ax[1].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8313dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W = img.shape[:2]\n",
    "\n",
    "with torch.no_grad():\n",
    "    enc = processor(images=[img], return_tensors=\"pt\", do_rescale=False)\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "    outputs = model(**enc)\n",
    "\n",
    "# (height, width) 순서!  그리고 CPU NumPy로 변환\n",
    "predicted_map = processor.post_process_semantic_segmentation(\n",
    "    outputs, target_sizes=[(H, W)]\n",
    ")[0]\n",
    "if isinstance(predicted_map, torch.Tensor):\n",
    "    predicted_map = predicted_map.detach().cpu().numpy().astype(np.uint8)\n",
    "seg = predicted_map  # (H,W) np.uint8\n",
    "# 팔레트\n",
    "color_palette = [10, 90]\n",
    "palette = np.asarray(color_palette, dtype=np.uint8)\n",
    "\n",
    "# 색상화\n",
    "color_seg = np.zeros((H, W, 1), dtype=np.uint8)\n",
    "for label, color in enumerate(palette):\n",
    "    color_seg[seg == label] = color\n",
    "\n",
    "# GT\n",
    "with rasterio.open(label_name) as src:\n",
    "    img_label = src.read(1).astype(np.float32)\n",
    "img_label = img_label / img_label.max()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16,8))\n",
    "ax[0].imshow(color_seg);       ax[0].axis('off')\n",
    "ax[1].imshow(img_label); ax[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, torch, rasterio, matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\n",
    "# 0=검정, 1=노랑\n",
    "cmap = ListedColormap([[0,0,0], [1,1,0]])\n",
    "norm = BoundaryNorm([-0.5, 0.5, 1.5], cmap.N)\n",
    "\n",
    "# --- 1) 예측 (이미 구한 predicted_map 사용 가정: (H,W) 정수 레이블) ---\n",
    "if isinstance(predicted_map, torch.Tensor):\n",
    "    pred_idx = predicted_map.detach().cpu().numpy().astype(np.int64)\n",
    "else:\n",
    "    pred_idx = np.asarray(predicted_map).astype(np.int64)\n",
    "\n",
    "# 모델이 2클래스인지 점검\n",
    "uniq_pred = np.unique(pred_idx)\n",
    "print(\"[pred unique]\", uniq_pred)\n",
    "\n",
    "# 필요시: 예측이 0/1 외의 값을 갖는다면 2클래스 작업에 맞게 클램프\n",
    "pred_idx = np.clip(pred_idx, 0, 1).astype(np.uint8)\n",
    "\n",
    "# --- 2) GT 10/90 -> 0/1 매핑 (flip_gt=True로 바꿔서 반전 테스트 가능) ---\n",
    "with rasterio.open(label_name) as src:\n",
    "    gt_raw = src.read(1)\n",
    "\n",
    "vals = np.unique(gt_raw)\n",
    "print(\"[gt unique]\", vals)\n",
    "\n",
    "# 기본: 90을 1(양성)로 매핑. (반전하려면 flip_gt=True)\n",
    "flip_gt = False\n",
    "if flip_gt:\n",
    "    gt_idx = (gt_raw == 10).astype(np.uint8)\n",
    "else:\n",
    "    gt_idx = (gt_raw == 90).astype(np.uint8)\n",
    "\n",
    "\n",
    "# --- 4) 정량 확인: IoU/혼동행렬 ---\n",
    "def iou_binary(p, t):\n",
    "    inter = np.logical_and(p==1, t==1).sum()\n",
    "    union = np.logical_or (p==1, t==1).sum()\n",
    "    bg_inter = np.logical_and(p==0, t==0).sum()\n",
    "    bg_union = np.logical_or (p==0, t==0).sum()\n",
    "    iou_fg = inter / (union + 1e-9)\n",
    "    iou_bg = bg_inter / (bg_union + 1e-9)\n",
    "    acc    = (p==t).mean()\n",
    "    return iou_fg, iou_bg, acc, inter, union\n",
    "\n",
    "iou1, iou0, acc, inter, union = iou_binary(pred_idx, gt_idx)\n",
    "print(f\"[metrics] IoU(fg=1)={iou1:.4f}, IoU(bg=0)={iou0:.4f}, Acc={acc:.4f}  (inter={inter}, union={union})\")\n",
    "\n",
    "# --- 6) 시각화: 동일 팔레트 + 윤곽선 ---\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "ax[0].imshow(pred_idx, cmap=cmap, norm=norm); ax[0].axis('off'); ax[0].set_title(\"Prediction\")\n",
    "ax[1].imshow(gt_idx,   cmap=cmap, norm=norm); ax[1].axis('off'); ax[1].set_title(\"Ground Truth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7e8f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = np.logical_and(pred_idx==1, gt_idx==1)\n",
    "TN = np.logical_and(pred_idx==0, gt_idx==0)\n",
    "FP = np.logical_and(pred_idx==1, gt_idx==0)  # 배경인데 예측이 1\n",
    "FN = np.logical_and(pred_idx==0, gt_idx==1)  # 양성인데 예측이 0\n",
    "\n",
    "tp = TP.sum(); tn = TN.sum(); fp = FP.sum(); fn = FN.sum()\n",
    "print(f\"TP={tp}, FP={fp}, FN={fn}, TN={tn}\")\n",
    "\n",
    "# IoU / Precision / Recall (양성=1 기준)\n",
    "iou_fg = tp / (tp + fp + fn + 1e-9)\n",
    "precision = tp / (tp + fp + 1e-9)\n",
    "recall    = tp / (tp + fn + 1e-9)\n",
    "# 배경 IoU\n",
    "iou_bg = tn / (tn + fp + fn + 1e-9)\n",
    "\n",
    "print(f\"IoU_fg={iou_fg:.4f}, IoU_bg={iou_bg:.4f}, Precision={precision:.4f}, Recall={recall:.4f}\")\n",
    "\n",
    "# --- 에러 지도 시각화 ---\n",
    "# 흰 바탕에 FP=빨강, FN=파랑, TP=초록, 나머지(TN)=흰색\n",
    "H, W = pred_idx.shape\n",
    "canvas = np.ones((H, W, 3), dtype=np.float32)  # white\n",
    "canvas[TP] = [0.0, 0.8, 0.0]   # green\n",
    "canvas[FP] = [0.9, 0.1, 0.1]   # red\n",
    "canvas[FN] = [0.1, 0.4, 0.9]   # blue\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "ax[0].imshow(pred_idx, cmap=\"gray\"); ax[0].set_title(\"Prediction (0/1)\"); ax[0].axis('off')\n",
    "ax[1].imshow(gt_idx,   cmap=\"gray\"); ax[1].set_title(\"Ground Truth (0/1)\"); ax[1].axis('off')\n",
    "ax[2].imshow(canvas);              ax[2].set_title(\"Error map: FP(red), FN(blue), TP(green)\"); ax[2].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35c669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
